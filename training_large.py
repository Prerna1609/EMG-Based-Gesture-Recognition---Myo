# -*- coding: utf-8 -*-
"""Training_Large.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OV1obunYm15SoA0yMzxr7x4uTuDwhOH7
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, BatchNormalization, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

# For reproducibility
np.random.seed(42)

# Load the CSV dataset
df = pd.read_csv('/content/Final_Dataset.csv')

# Display the first few rows and the column names to understand the structure.
print("Dataset Head:")
print(df.head())
print("\nColumns in dataset:")
print(df.columns)

sensor_columns = df.columns[:-1]
label_column = df.columns[-1]

print("Sensor Columns:", sensor_columns.tolist())
print("Label Column:", label_column)

# Normalize the sensor data using StandardScaler
scaler = StandardScaler()
df[sensor_columns] = scaler.fit_transform(df[sensor_columns])

# Parameters for segmentation
window_size = 50  # number of time steps per segment
step_size = 10    # sliding window step size

def create_segments(dataframe, window_size, step_size):
    """
    Segments the continuous dataframe into samples of fixed window size.
    For each segment, the label is assigned as the mode (most common value) of the label column.
    """
    segments = []
    labels = []
    for start in range(0, len(dataframe) - window_size + 1, step_size):
        end = start + window_size
        segment = dataframe.iloc[start:end][sensor_columns].values  # shape: (window_size, num_sensors)
        # Assign the label as the mode (most frequent label in the segment)
        label = dataframe.iloc[start:end][label_column].mode()[0]
        segments.append(segment)
        labels.append(label)
    return np.array(segments), np.array(labels)

# Create segments and labels
X, y = create_segments(df, window_size, step_size)
print("Segmented data shape (samples, time_steps, features):", X.shape)
print("Segmented labels shape:", y.shape)

# Check label distribution to ensure all classes are well-represented
print("\nLabel distribution:")
print(pd.Series(y).value_counts())

# Encode the gesture labels into integers
le = LabelEncoder()
y_encoded = le.fit_transform(y)
num_classes = len(np.unique(y_encoded))
print("Number of gesture classes:", num_classes)

# Convert labels to one-hot encoding
y_categorical = to_categorical(y_encoded, num_classes=num_classes)

# Split the data into training and test sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)
print("Training data shape:", X_train.shape)
print("Test data shape:", X_test.shape)

# Build the model using Keras Sequential API
model = Sequential()

# Input Layer & First Convolutional Block
model.add(Conv1D(filters=64,
                 kernel_size=3,
                 activation='relu',
                 padding='same',
                 input_shape=(window_size, len(sensor_columns))))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=2))  # Downsamples the temporal dimension

# Second Convolutional Block
model.add(Conv1D(filters=128,
                 kernel_size=3,
                 activation='relu',
                 padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=1))  # Maintains temporal dimensions (for block consistency)

# Classification Block
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.3))  # Dropout to reduce overfitting
model.add(Dense(num_classes, activation='softmax'))  # Output layer

# Compile the model
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

# Display the model architecture
model.summary()

# Define training parameters
batch_size = 32
epochs = 10

# Define callbacks for early stopping and learning rate reduction
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)

# Optionally, you can also save the best model during training
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)

# Train the model
history = model.fit(X_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    validation_split=0.2,
                    callbacks=[early_stop, reduce_lr, checkpoint],
                    shuffle=True,
                    verbose=1)

# Evaluate model performance on the test set
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print("Test Accuracy: {:.2f}%".format(test_acc * 100))

# Plot training and validation accuracy and loss

plt.figure(figsize=(12, 5))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss plot
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import itertools

# Option 2: Evaluate on the entire dataset
X_eval, y_eval = X, y_categorical


# Predict probabilities and then convert to class labels
y_pred_probs = model.predict(X_eval)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = np.argmax(y_eval, axis=1)

# Print distributions for debugging
unique_true, counts_true = np.unique(y_true, return_counts=True)
unique_pred, counts_pred = np.unique(y_pred, return_counts=True)
print("True label distribution:", dict(zip(le.classes_, counts_true)))
print("Predicted label distribution:", dict(zip(le.classes_, counts_pred)))

# Compute the confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(8, 8))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.colorbar()

# Use the original class names from the label encoder
classes = le.classes_
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation=45, ha="right")
plt.yticks(tick_marks, classes)

# Add counts in each cell
thresh = cm.max() / 2.
for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, format(cm[i, j], "d"),
             horizontalalignment="center",
             color="white" if cm[i, j] > thresh else "black")

plt.tight_layout()
plt.ylabel("True Label")
plt.xlabel("Predicted Label")
plt.show()